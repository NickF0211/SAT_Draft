\section{Find i-uip Clause} \label{sec:i-uip}
Classical CDCL SAT solvers analyze each conflict with 1-UIP resolution learning scheme to derive an asserting clause $\oneUIPClause$. Even though prior studies shows that further resolutions on $\oneUIPClause$ against the assertion trail $\assertionTrail$ cannot reduce the clause's literal block distance ($\LBD$), the resolutions could potentially reduce the clause's size. Clause size is an important quality measurement because a smaller clause 1) consumes less memory, 2) requires less steps to force a literal and 3) decreases the size of future conflict clauses.

Inspired by both $\LBD$ and clause size, i-UIP resolution learning attempts to resolve away literals in $\oneUIPClause$ against the assertion trail $\assertionTrail$ to minimize the clause size without increasing the clause's $\LBD$. The goal of i-UIP learning is to find a clause $\iUIPClause$ whose literals are either the unique implication points in their respective decision levels or are directly implied by literals from a foreign decision level. Alg. ~\ref{alg:i-uip} is a pseudo-code implementation of i-UIP learning. 


\begin{algorithm}[t]
\caption{\IUIP}\label{alg:i-uip}
\begin{algorithmic}[1]
\Require  $\oneUIPClause$ is a valid and minimized 1-UIP clause
\Require  $\assertionTrail$ is a valid assertion trail
\Procedure{\IUIP}{$\oneUIPClause, \assertionTrail$} 
    \State $\iUIPClause \gets \oneUIPClause $ \Comment{initialize i-UIP clause} \label{ln:init}
    \State \code{DecisionLvs} $\gets$ Decision levels in $\oneUIPClause$ in descending order
\For{$i \in \code{DecisionLvs}$} \label{ln:iterDecisionLvls}
    \State $L_{i} \gets \Set{l}{\code{level}(l) = i \wedge l \in \iUIPClause}$
    \While{$|\code{unmarked}(L_i)| > 1$} \label{ln:uip}
        \State $p \gets \text{lit with the highest trail position in } L_{i} $ \label{ln:trailaccess}
        \If{$\exists q \in \code{Reason}(\neg{p}, \assertionTrail) \cdot \code{level}(q) \not\in$ \code{DecisionLvs}} \label{ln:resolvable}
            \If{$\IUIPPURE$}
                \State Unresolve literals at level $i$  \label{ln:unresolve}
                \State Go to the next decision level $i$ \label{ln:abondon}
            \ElsIf{$\IUIPMIN$}
                \State $\code{Mark}(p)$ \label{ln:mark}
            \EndIf
        \Else 
            \State  $\iUIPClause \gets (\oneUIPClause ) \bowtie \code{Reason}(\neg{p}, \assertionTrail)$
           \State  $\code{Update}(L_{i})$
        \EndIf
    \EndWhile
\EndFor \label{ln:endCompleteResolution}
\\
\If{$\IUIPPURE$}\label{ln: 2ndMinStart}   \Comment{$\iUIPClause$ from $\IUIPPURE$ can be minimized}
    \State $\iUIPClause  \gets \textbf{Minimize}(\iUIPClause)$ 
\EndIf\label{ln: 2ndMinEnd}
\\
\If{$|\iUIPClause| < |\oneUIPClause|$} \label{ln:checkSize}
    \State \textbf{return} $\iUIPClause$
\Else
    \State \textbf{return} $\oneUIPClause$
\EndIf  \label{ln:cSizeCheck}
\EndProcedure
\end{algorithmic}
\end{algorithm}

The algorithm $\IUIP$ computes $\iUIPClause$ from (line~\ref{ln:init} to \ref{ln:endCompleteResolution}), and then returns the smaller clause between 
$\oneUIPClause$ and $\iUIPClause$ (line~\ref{ln:checkSize} to \ref{ln:cSizeCheck}). 
The algorithm first initializes $\iUIPClause$ with the minimized\cite{} $\oneUIPClause$. Next, the algorithm iterate through the decision levels of $\iUIPClause$ in descending order (line~\ref{ln:iterDecisionLvls}) and tries to find the unique implication point in each level (line~\ref{ln:uip}). Since $\IUIP$ needs to preserve $\iUIPClause$'s $\LBD$ during resolutions, before resolving away a target literal $p$, the algorithm preemptively checks whether the reasoning clause of $\neg{p}$ contains any literal $q$ from an unseen decision level (line~\ref{ln:resolvable}). If such a literal $q$ exists, then the algorithm cannot find the unique implication point (UIP) at the current level $i$ because resolving away $p$ will introduce $q$ into the clause and increases $\LBD$. One solution is to abandon level $i$ by reverting back to the state before any literal at level $i$ is resolved away (line~\ref{ln:unresolve}), and then move on to the next level (line~\ref{ln:abondon}). We denote $\IUIP$ learning with this solution as $\IUIPPURE$. The final $\iUIPClause$ produced by $\IUIPPURE$ will contains exactly one literal for levels where the LBD-persevering UIP exist. However, $\IUIPPURE$ does not minimize the number of literals in the decision levels where the LBD-preserving UIP does not exist, and clause minimization techniques\cite{} can be used to further reduce the clause size (line~\ref{ln: 2ndMinStart} to \ref{ln: 2ndMinEnd}). 

  When $\IUIP$ cannot resolve away a literal $p$ without increasing $\iUIPClause$'s LBD, instead of skipping the entire decision level $i$, another solution $\IUIPMIN$  simply marks literal $p$ and keep it  in $\iUIPClause$ (line~\ref{ln:mark}). The learning scheme then continue the resolutions at level $i$. $\IUIPMIN$ will ignore all the unsolvable literals and find the "local" unique implication point at every decision level.  The algorithm terminates when there is exactly one unmarked literal left for each decision level of $\iUIPClause$, representing the local unique implication points. The clause $\iUIPClause$ learnt by
  $\IUIPMIN$ is minimized\footnote{clause size cannot be reduced via minimization techniques\cite{}.}, we defer the proof of minimization to Appendix~\ref{}.  

\begin{example}
Consider the assertion trail $\assertionTrail = {..a_2,b_2,c_2,d_2..e_5,f_5,g_5,h_6,i_6,j_6,k_6..}$ where literals $a_2 ... k_{6}$ are asserted in order with decision level shown as the underscript. Let $L_{ext}$ denote the set of literals on $\assertionTrail$ outside of decision level 2,5,6 and 10, and $l^{*}_{ext}$ be some literal in $L_{ext}$. The reasoning clauses involved in building $\assertionTrail$ are:
\begin{enumerate}
  \item[] $C_k = \{\neg{f_5} \vee \neg{j_6} \vee k_6\}$,
  $C_j = \{\neg{f_5} \vee \neg{i_6} \vee j_6\}$, 
  $C_i = \{\neg{e_5} \vee \neg{h_6} \vee i_6\}$,
  $C_g = \{\neg{a_2} \vee \neg{f_5} \vee g_5\}$
  $C_f = \{\neg{l^{*}_{ext}} \vee \neg{e_5} \vee f_5\}$,
  $C_d = \{\neg{b_2} \vee \neg{c_2} \vee d_2\}$,
  $C_c = \{\neg{a_2} \vee \neg{b_2} \vee c_2 \}$,
  $C_b = \{\neg{l^{*}_{ext}} \vee \neg{a_2} \vee b_2\}$
\end{enumerate}
Suppose 1-UIP learning produces clause 
$\oneUIPClause = \{\neg{m_{10}} \vee \neg{k_6} \vee \neg{j_6} \vee \neg{i_6} \vee \neg{h_6} \vee \neg{g_5} \vee \neg{d_2} \vee \neg{c_2}\}$ where $\neg{k_{10}}$ is the UIP from the conflicting level. $\IUIP$ first tries to find the UIP for level 6 by resolving $\oneUIPClause$ with $C_k$, $C_j$ and, $C_i$ in order and produces clause 
$\iUIPClause = \{\neg{m_{10}} \vee \neg{h_6} \vee \neg{g_5} \vee \neg{f_5} \vee \neg{e_5} \vee \neg{d_2} \vee  \neg{c_2} \}$ where $\neg{h_6}$ is the UIP.

$\IUIP$ attempts to find the UIP for level 5 by resolving $C_i$ with 
$C_g$ and $C_f$ in order. However, resolving with $C_f$ will introduce $\neg{l^{*}_{ext}}$ into 
$C_i$ and increase the clause's LBD. $\IUIPPURE$ undoes all resolutions at level 5 and skips the entire level, whereas $\IUIPMIN$ only skips the resolution with $C_f$ and tries to find the UIP by ignoring $\neg{f_5}$.  After level 5, $\IUIPPURE$ preserves the same clause and $\IUIPMIN$ produces $\iUIPClause = \{\neg{m_{10}} \vee \neg{h_6} \vee \neg{f_5} \vee \neg{e_5} \vee \neg{d_2} \vee \neg{c_2} \vee \neg{a_2}\}$ where $\neg{e_5}$ is the UIP when $\neg{f_5}$ is ignored. The same $\IUIP$ procedure is then performed on level 2 to produce the final $\iUIPClause$. 

$\IUIPPURE$ learned $\iUIPClause = \{\neg{m_{10}} \vee \neg{h_6} \vee \neg{g_5} \vee \neg{f_5} \vee \neg{e_5} \vee \neg{d_2} \vee  \neg{c_2} \}$ and reduces the learnt clause size by 1. $\IUIPMIN$ learned $\iUIPClause = \{\neg{m_{10}} \vee \neg{h_6} \vee \neg{f_5} \vee \neg{e_5} \vee \neg{b_2} \vee \neg{a_2}\}$ and reduces the learnt clause size by 2. 
\end{example}

The core algorithm of $\IUIP$ as a clause reduction technique is simple. However, to make the algorithm a practical learning scheme that is compatible with modern SAT solvers, a number of issues need to be addressed. The rest of the section details the key optimizations and augmentations of $\IUIP$ as a practical clause learning scheme.

\subsection{Control i-UIP Learning}
This simple and greedy $\IUIP$ learning scheme can produce significantly smaller clause. However, when $\IUIP$ does not reduce the clause size, the cost of the additional resolution steps will hurt the solver's performance. Since resolution cannot reduce a clause's $\LBD$, The maximum size reduction from $\IUIP$ is the difference between the clause's size and $\LBD$, denote as the clause's gap value ($\GAP(\oneUIPClause) = |\oneUIPClause| - \LBD(\oneUIPClause)$). For an 1-UIP clause with a small $\GAP$, applying $\IUIP$ is unlikely to achieve cost effective results. Therefore, we propose a heuristic based approach to to enable and disable $\IUIP$ learning based on input clause's $\GAP$.

\begin{algorithm}[t]
\caption{Control-$\IUIP$}\label{alg:enableCondition}
\begin{algorithmic}[1]
\Require  $\oneUIPClause$ is a valid 1-UIP clause
\Require  $ t_{gap} \ge 0$ is a dynamically calculated gap threshold
\Procedure{Control-$\IUIP$}{$\oneUIPClause,  t_{gap}$} 
    \State $Gap \gets |\oneUIPClause| - \textit{LBD}(\oneUIPClause)$
     \If{$Gap > t_{gap}$} \label{ln:compareGap}
        \State $\iUIPClause \gets \IUIP(\oneUIPClause, \phi)$
        \State \textcolor{gray}{\textbf{I-UIP-Greedy}($\iUIPClause$ , $\oneUIPClause$)}  \Comment{additional clause selection policy, see sec~\ref{sec:greedy}}
        \If {$|\iUIPClause| < |\oneUIPClause|$} 
            \State $\text{Succeed} \gets \text{Succeed}+1$ \label{ln:updateS}
        \EndIf
        \State $\text{Attempted} \gets \text{Attempted}+1$ \label{ln:updateA}
     \EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}

Alg.~\ref{alg:enableCondition} compares the input $\oneUIPClause$'s $\GAP$ against a floating target threshold $t_{gap}$ (line~\ref{ln:compareGap}). The threshold $t_{gap}$ represent the expected minimal $\GAP$ required for $\oneUIPClause$ to achieve a predetermined success rate (80\%) from performing $\IUIP$ learning. 

The gap threshold $t_{gap}$ is initialized to 0, and is updated at every restart based on $\IUIP$'s success rate from the previous restart interval. More specifically, the algorithm collects the statistics of the number of $\IUIP$ learning attempted (line~\ref{ln:updateA} in alg.~\ref{alg:enableCondition}) and the number of attempts succeed (line~\ref{ln:updateS}) for each restart interval, and use them to calculate the success rate. If the success rate is below 80, the threshold $t_{gap}$ is increased to restrict $\IUIP$for the next restart interval. On the other hand, the threshold is decreased to encourage more aggressive $\IUIP$ learning for the next restart interval.

\[
    t_{gap}=
\begin{cases}
    t_{gap} + 1& \text{if } \frac{\text{Succeed}}{\text{Attempted}} < 0.8\\
    max(t_{gap} - 1, 0),              & \text{otherwise}
\end{cases}
\]

\subsection{Early stop i-UIP Learning}
At any point of $\IUIPMIN$ learning, if the number of marked literals in $\iUIPClause$ (literals which are forced into the clause to preserve $\LBD$) exceeds the input clause $\oneUIPClause$'s $\GAP$, we can abort $\IUIP$ learning for $\oneUIPClause$ because the size of the final $\iUIPClause$ is at least the size of $\oneUIPClause$.  The early stopping rule prevents solver from wasting time on traversing a large implication graph when $\IUIP$ has already failed. 

The early stopping rule may prematurely stops $\IUIPPURE$ learning which would otherwise succeed because $\iUIPClause$ can be further minimized. However, the experiments indicates such false negative cases are rare, and the early stopping rule generally helps the learning scheme's performance.

\subsection{Greedy Active Clause Selection} \label{sec:greedy}
Even though $\IUIP$ learning can reduce the size of the learnt clause $\iUIPClause$, but it may introduce literals with low variable activity into $\iUIPClause$.  Inactive literals prevents the clause from being asserted to force literal implication. Therefore, a practical clause learning scheme should consider both size and variable activity. We propose an optional extension $\IUIPGreedy$ to filter out inactive $\oneUIPClause$.  

\begin{algorithm}[t]
\caption{$\IUIPGreedy$}\label{alg:greedySelection}
\begin{algorithmic}[1]
\Require  $\iUIPClause$ is a valid i-UIP clause
\Require  $\oneUIPClause$ is a valid 1-UIP clause
\Procedure{$\IUIPGreedy$}{$\iUIPClause, \oneUIPClause$} 
   \If{$|\iUIPClause| < |\oneUIPClause| \wedge (\code{AvgVarAct}(\iUIPClause) > \code{AvgVarAct}(\oneUIPClause)) $} \label{ln:checkSizeAndActivity}
    \State \textbf{return} $\iUIPClause$
\Else
    \State \textbf{return} $\oneUIPClause$
\EndIf  \label{ln:cSizeCheck}
\EndProcedure
\end{algorithmic}
\end{algorithm}

After computing the $\iUIPClause$, $\IUIPGreedy$ compares both the size and the average variable activity for $\oneUIPClause$ and $\iUIPClause$ (alg.~\ref{alg:greedySelection} at line~\ref{ln:checkSizeAndActivity}). The algorithm learns $\iUIPClause$ if the clause has smaller size and higher average variable activity. 

\subsection{Adjust Variable Activity} \label{sec: varajust}
Two popular branching heuristics for modern SAT solvers are VSIDS and LBR. Both heuristics increase 
the variable activity for all literals involved in resolutions during 1-UIP learning. Since $\IUIP$ extends 1-UIP with deeper resolutions against the trail, the variables activities for the fresh literals involved in the additional resolution steps need to be adjusted as well. We purpose two optional schemes for adjusting variable activities, $\IUIPActive$ and $\IUIPDist$. 

After learning $\oneUIPClause$ from 1-UIP scheme, $\IUIPActive$ collects all literals appear  in the the $\IUIP$ clause $\iUIPClause$, and increase their variable activity uniformly according to the current branching heuristic if the literals' variable activity have not yet been bumped during 1-UIP. The scheme does not bump variable activities for transient literals that are resolved away at non-conflicting level because, unlike transient literals at conflicting level, these literals cannot be re-asserted after the immediate backtracking. Notice that other post-analyze extensions such as Reason Side Rate (RSR) and Locality\cite{} can be applied after $\IUIPActive$.

\[ \forall l \in  \iUIPClause \cdot NotBumped(var(l)) \implies  bumpActivity(var(l)) \]

$\IUIPDist$ collects literals appear exclusively in $\iUIPClause$ and bump their variable activity uniformly. It also find all the literals in $\oneUIPClause$ that are resolved away during $\IUIP$, and unbump their variable activity if they have been bumped during 1-UIP. The unbumped literals are no longer in the learned clause $\iUIPClause$, keep their variable activity bumped does not help solver to use $\iUIPClause$.

\[ \forall l_{i} \in  \iUIPClause \setminus \oneUIPClause \cdot bumpActivity(var(l_{i})) 
\]
\[ \forall l_{1} \in  \oneUIPClause \setminus \iUIPClause \cdot unbumpActivity(var(l_{1})) 
\]


\subsection{Integrate with Chronological Backtracking}
In SAT solver with chronological backtracking, literals on the assertion trail $\assertionTrail$ are not always sorted by decision levels. This change imposes a challenge to $\IUIP$ learning since the previous implementation relies on solver's ability to efficiently access all literals from any decision level in descending trail order (alg.~\ref{alg:i-uip} line~\ref{ln:trailaccess}). 

To mitigate this challenge, we modify the solver to track the precise trail position of all asserted literals with a single vector. We then build a priority queue $lit\_Order$ to manage literal's resolution order. The queue $lit\_Order$  prioritizes literals with higher decision level, and it favors literal with higher trail position when decision levels are tied. The order of $lit\_Order$  represents the correct resolution order of $\IUIP$ because 1) an asserted literal's decision level is the maximum level among all of its reasoning literals, and 2) an asserted literal always appears higher in the trail than its reasoning literal.  

\begin{algorithm}[t]
\caption{\IUIP-CB}\label{alg:i-uip-CB}
\begin{algorithmic}[1]
\Require  $\oneUIPClause$ is a valid 1-UIP clause
\Require  $\assertionTrail$ is a valid assertion trail
\Require  $lit\_Order$ is a priority queue
\Procedure{\IUIP-CB}{$\oneUIPClause, \assertionTrail, lit\_Order $} 
    \State ...
    \State $\iUIPClause \gets \oneUIPClause $ \Comment{initialize i-UIP clause}
    \State forall $l \in \oneUIPClause \cdot \code{Enqueu}(lit\_Order, l)$  \label{ln:enqueue}
    \State \code{DecisionLvs} $\gets \Set{\code{level}(l)}{l \in \oneUIPClause}$
    \While{$lit\_Order \neq \emptyset$} \label{ln:pop}
        \State $p \gets \code{dequeu}(lit\_Order, l) $ \label{ln:dequeue}
        \If{$\exists q \in \code{Reason}(\neg{p}, \assertionTrail) \cdot \code{level}(q) \not\in$ \code{DecisionLvs} \\ $\vee p$ is the last reminaing lit in its decision level } \label{ln:resolvable}
          \State Pass
        \Else 
            \State  $\iUIPClause \gets (\oneUIPClause ) \bowtie \code{Reason}(q, \assertionTrail)$
            \State   forall $l \in \code{Reason}(q, \assertionTrail) \cdot \code{Enqueu}(lit\_Order, l)$ \label{ln:reequeue}
        \EndIf
    \EndWhile
\State ...
\EndProcedure
\end{algorithmic}
\end{algorithm}

Alg.~\ref{alg:i-uip-CB} is the pesudo-code implementation of the augmented $\IUIP$ for chronological backtracking with the priority queue $lit\_Order$. The algorithm first populates $lit\_Order$ with all literals in $\oneUIPClause$ (line~\ref{ln:enqueue}), and then continuously pops literals until the queue is empty (line~\ref{ln:pop}). When a literal is resolved away, all of its reasoning literals are added into $lit\_Order$ if they are not already in the queue (line~\ref{ln:reequeue}). The algorithm will always terminate because a literal cannot entered the queue twice (guaranteed by the properties of the trail order) and there are finite amount of literals on the trail.